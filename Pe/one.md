## 步骤一
详细步骤：

本发明使用到的参数信息列表如下：

- $S\{s_0,s_1,...,\}$: 环境中博弈的参与者列表，其中 $s_0$ 表示自车
- $V\{v_0,v_1,...,\}$: 环境中每个参与者对应的速度信息，其中 $v_0$ 表示自车的当前速度
- $\Theta\{\theta_0,\theta_1,...\}$: 环境中每个参与者对应的速度朝向信息，其中 $\theta_0$ 表示自车当前的速度朝向。
- $L\{l_0,l_1,...\}$: 环境中每个参与者的车长参数，其中 $l_0$ 表示自车的车长。
- $W\{w_0,w_1,...\}$: 环境中每个参与者的车宽参数，其中 $w_0$ 表示自车的车宽。
- $P\{p_0,p_1,...\}$: 环境中每个参与者的中心笛卡尔坐标系，其中 $p_0$ 表示自车的中心坐标。
- $P^s\{p^s_0,p^s_1,...\}$: 环境中每个参与者的中心Frenet坐标，其中 $p^s_0$ 表示自车的中心Frenet坐标
- $T\{t_0,t_1,...\}$: 环境中每个障碍物的轨迹信息，其中 $t_0$ 表示自车的规划轨迹。其余参与者的信息由预测获取。
- $TTC\{ttc_{0,1},ttc_{0,2},...ttc_{0,n-1}\}$: 环境中任一交互参与者与自车的的ttc信息。
- $THW\{thw_1,...\}$: 环境中每个障碍物到达相对于自车冲突点所需要的时间。
- $D\{d_0,d_0...\}$: 环境中每个障碍物的最终决策，可以表示为 t 的函数，其分布可以简化收敛于二值分布，即 $d_i(t) \in \{0,1\}$,其中 0\1分别对应参与者的让行，抢行决策。
- $D_{ego}\{d_0,...\}$: 环境中针对每个障碍物，自车的决策概率，可以表示为 t 的函数，其分布可以简化收敛于二值分布，即 $d_i(t) \in \{0,1\}$,其中 0\1分别对应参与者的让行，抢行决策。


步骤一：预处理环境信息与自车状态。

1.1 环境信息获取
本发明中首先从感知测获取障碍物的感知信息，包括障碍物的速度 v, 中心位置 $p_c$, 形状信息包括车长： $l$， 车宽：$w$； 从预测获取障碍物的预期轨迹 $t$。
 
1.2 初步障碍物筛选 
如下所示为一些常规的无保护转向路口，自车转向场景下，需要处理的障碍物主要包括图示区域来车。为了减少计算负荷，首先基于简单的规则对障碍物进行一些筛选，从而缩小关注障碍物列表元素数量。
设定过滤规则如下：
- 自车规划PATH与障碍物预测轨迹无空间交叠。

假定过滤后的障碍物列表为 $S$, 

1.3 需求参数计算

ttc = 

thw_front = 

thw_back = 

步骤二：历史信息处理：

历史信息分为历史决策信息与历史环境信息。两种历史信息使用方式如下：

**历史环境信息**:
历史环境信息主要存储对应障碍物的$thw^h_front$, $thw^h_back$ 信息，如果信息不可用，则可保留对应障碍物的历史信息。
历史环境信息使用方式如下：
$$
    {thw}_{front} =\beta \times {thw}^h_{front} + (1-\beta) \times {thw}_{front}
$$

式中 $p$ 表示置信参数，表示历史信息置信程度，后通过标定或学习方案获取。

**历史决策信息**:
历史决策信息表示为 $d^h_i$，使用方式为对当前帧决策做初始决策增强。
增强方式如下：
$$
d_i = d_i + \alpha d^h_i
$$
具体使用策略可以后续会进一步解释。

## 步骤二：基于已知信息构建博弈各参与者的单帧收益函数。

2.1 环境中每个障碍物的决策离散化

为了将问题建模为方便进行优化求解的形式，对优化变量 $d_i$ 进行离散化，其物理意义可以理解为参与者的决策概率，例如 $d_i = 0.7$，即表示参与者$i$在该场景中抢行的概率为0.7。于是d_i 的结果即参与者的抢行概率, $1-d_i$ 即表示参与者的让行概率。

2.2 决策收益函数构建

博弈收益函数的设计基于以下几条原则：

- 决策收益函数指博弈参与者做出某种决策后的收益。因此表示为 $d_i$ 决策概率的函数，
- 本发明决策收益函数的设计仅进行抢、让的决策，不进行最终速度曲线的生成，因此代价函数的组成只考虑安全性与效率。
- 自车的决策最终只能直接影响自车的轨迹，因此只需要考虑每个障碍物与自车的决策结果，而不需要考虑非自车障碍物之间的决策。




于是每个障碍物与自车决策收益可以表示为：
$$
对于交通参与者 i, 其决策变量为 d_i, 定义交通参与者在与自车博弈时，收益函数: \\ 
f(d_i) = c_s(thw_0,d_i) +  c_e(thw_0,d_i) \\   
$$

以任一对博弈者为例，对任一障碍物 $o_i$，依托步骤一计算的到的相应信息 ,其与自车的收益矩阵可以简述为
          o_0
     -1/ti - ti, -1/t0 - t0  |  1/ti + ti, 1/t0 - at0
o_i  ki - ati , k0 + at0  | 1/ti - ti, 1/t0 - at0

为了进一步考虑车身长度影响，对thw 进行变形，分别构建参与者通过碰撞域起点的thw 为 thw_f 和 参与者完全通过碰撞域的thw 为 thw_b， 分别用来替换收益矩阵中的安全性参数与效率参数，于是收益矩阵转换为下面的形式。

$$
\begin{table}
%     o_0 
%      -1/tf_i - tb_i,& -1/tf_0 - tb_0   1/tf_i + tb_i, 1/tf_0 - atb_0 \\
% o_i  1/tf_i - atb_i , 1/tf_0 + atb_0  | 1/tf_i - tb_i, 1/tf_0 - atb_0
\end{table}
$$

将支付矩阵转换为收益函数形式，
$$
f(d_i) = d_i \times d_0 (-1/{tf}_0 - {tb}_0) + d
$$

$$
Cost_i =\int p(d(S_0,S_1,S_2,...)) \times f(d(S_0,S_1,...)) \space ds
$$

式中，$p(d(x))$ 为障碍物决策概率密度函数，$f(d(x))$ 为障碍物决策收益函数， $d(x)$表示障碍物的决策函数。


## 步骤三：基于各参与者收益函数求解各参与者纳什均衡点。

基于每个障碍物的收益函数，可以求解每个障碍物与自车的均衡点策略，即：

$$
\frac{\partial Cost_i}{\partial d_i} = 0 \\

d_i = \arg \max Cost_i
$$

求解得到每个障碍物的最优决策期望，即为纳什均衡点。

## 步骤四：基于收益均衡点与上一帧自车决策结果构建当前帧初始规划点。

设定各障碍物上一帧决策结果满足 $D^{-1}\{d_0,d_0...\}$，上一帧自车针对每个障碍物的最终决策为 $D_{ego}^{-1}\{d_0,...\}$，上一帧规划轨迹为 $T^{-1}\{t_0,t_1,...\}$。于是，当前帧初始规划点可以表示为上一帧规划轨迹对应的修正后的障碍物决策概率。

对于一个如下决策ST图，其中横轴表示规划时间，纵轴表示规划位置，图中的曲线表示上一帧规划结果，图中块表示障碍物。上一帧的 $d^{-1}_i$ 为图中红色文字，表示上一帧自车对该障碍物的决策概率，$d_i$ 为图中蓝色文字，表示当前帧自车对该障碍物的决策概率。图中曲线可以表示二值的最终决策，即障碍物位于曲线下方则表示让行，障碍物位于曲线上方则表示抢行。
### 4.1 定义决策转换的困难程度
决策转换的困难程度算法描述如下：a
```
Algorithm: DecisionTransformDifficulty
Input: 
    $d_i$: 当前帧自车对障碍物i的决策概率
    $d_i^{-1}$: 上一帧自车对障碍物i的决策概率
    $T^{-1}\{t_0,t_1,...\}$:上一帧规划的轨迹
Output:
    $d_i$: 当前帧自车对障碍物i的修正决策概率d_i

    1: B = [1..n]    // Initialize B as a list of n obstacles
    2: for i = 1 to n do
    3:    $T^{-1}_{rev}\{t_0,t_1,...\}$ = GenerateReverseDecisionTrajectory($d_i^{-1}$, $T^{-1}\{t_0,t_1,...\}$)
    4:    TransformNeedS = StaticTransformNeedS(T^{-1}_{rev}\{t_0,t_1,...\} ,$T^{-1}\{t_0,t_1,...\}$)
    5:    DiffcultSign = 2 * TransformNeedS / OccuptyTime(i)**2

Algorithm:GenerateReverseDecisionTrajectory
Input:
    $d_i^{-1}$: 上一帧自车对障碍物i的决策概率
    $T^{-1}\{t_0,t_1,...\}$:上一帧规划的轨迹
Output:
    $T^{-1}_{rev}\{t_0,t_1,...\}$:上一帧规划的轨迹的反向决策轨迹

    1: 


Algorithm:StaticTransformNeedS
```
### 4.2 决策增强
如果上一帧决策与本帧决策冲突，则需要对本帧决策进行增强，增强方式如下：
如果决策的困难程度超出自车a的界，则拒绝决策转换，否则进行决策增强：
增强方式为:



## 步骤六：基于已知决策推演生成自车规划结果。
决策已有的情况下，原规划问题可以转换为ST图上的凸优化问题，决策空间即为凸空间，因此以凸空间中界为参考，构建凸优化问题求解即可。


